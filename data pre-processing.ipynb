{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\chada\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import librosa\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from sklearn.metrics import accuracy_score,classification_report,confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define constants for data preprocessing\n",
    "num_mfcc_coefficients = 13  # Number of MFCC coefficients (adjust as needed)\n",
    "desired_shape = (457, num_mfcc_coefficients, 1)  # Replace with your model's input shape\n",
    "main_folder = ['belly_pain', 'burping', 'discomfort', 'hungry', 'tired']  # Replace with your class names\n",
    "folder_name = 'donateacry'  # Replace with your dataset folder path\n",
    "num_classes = len(main_folder)  # Number of classes\n",
    "\n",
    "# Initialize lists to store preprocessed data\n",
    "preprocessed_data = []\n",
    "preprocessed_labels = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to preprocess an audio file\n",
    "def preprocess_audio(audio_file, label):\n",
    "    # Load audio\n",
    "    audio, sr = librosa.load(audio_file, sr=None)\n",
    "\n",
    "    # Perform feature extraction (e.g., MFCCs)\n",
    "    mfccs = librosa.feature.mfcc(y=audio, sr=sr, n_mfcc=num_mfcc_coefficients)\n",
    "\n",
    "    # Normalize the MFCCs (optional but recommended)\n",
    "    mfccs = (mfccs - np.mean(mfccs)) / np.std(mfccs)\n",
    "\n",
    "    # Reshape or pad the MFCCs to match the desired input shape\n",
    "    num_frames = mfccs.shape[1]\n",
    "    if num_frames < desired_shape[0]:\n",
    "        mfccs = np.pad(mfccs, ((0, 0), (0, desired_shape[0] - num_frames)), mode='constant')\n",
    "    elif num_frames > desired_shape[0]:\n",
    "        mfccs = mfccs[:, :desired_shape[0]]\n",
    "\n",
    "    # Append the preprocessed data and label\n",
    "    preprocessed_data.append(mfccs.T[:, :, np.newaxis])  # Transpose the data\n",
    "    preprocessed_labels.append(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total audio files: 68\n"
     ]
    }
   ],
   "source": [
    "# Loop through each class folder and preprocess audio files\n",
    "for index, cls in enumerate(main_folder):\n",
    "    class_folder = os.path.join(folder_name, cls)\n",
    "    for file in os.listdir(class_folder)[:15]:\n",
    "        audio_file = os.path.join(class_folder, file)\n",
    "        preprocess_audio(audio_file, label=index)\n",
    "\n",
    "# Convert lists to NumPy arrays\n",
    "preprocessed_data = np.array(preprocessed_data)\n",
    "preprocessed_labels = np.array(preprocessed_labels)\n",
    "\n",
    "print(\"Total audio files:\", len(preprocessed_data))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\chada\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\keras\\src\\backend.py:1398: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\Users\\chada\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\keras\\src\\layers\\pooling\\max_pooling2d.py:161: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Define the CNN model\n",
    "model = keras.Sequential([\n",
    "    layers.Input(shape=desired_shape),  # Specify the input shape (e.g., (num_frames, num_features, num_channels))\n",
    "    \n",
    "    # Convolutional layers\n",
    "    layers.Conv2D(64, kernel_size=(3, 3), activation='relu', padding='same'),\n",
    "    layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "    layers.Conv2D(128, kernel_size=(3, 3), activation='relu', padding='same'),\n",
    "    layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "    layers.Conv2D(128, kernel_size=(3, 3), activation='relu', padding='same'),\n",
    "    layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "    \n",
    "    # Flatten the output\n",
    "    layers.Flatten(),\n",
    "    \n",
    "    # Fully connected layers\n",
    "    layers.Dense(128, activation='relu'),\n",
    "    layers.Dropout(0.3),  # Dropout layer to reduce overfitting\n",
    "    layers.Dense(num_classes, activation='softmax')  # Output layer with the number of classes\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\chada\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\keras\\src\\optimizers\\__init__.py:309: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d (Conv2D)             (None, 457, 13, 64)       640       \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2  (None, 228, 6, 64)        0         \n",
      " D)                                                              \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 228, 6, 128)       73856     \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPoolin  (None, 114, 3, 128)       0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 114, 3, 128)       147584    \n",
      "                                                                 \n",
      " max_pooling2d_2 (MaxPoolin  (None, 57, 1, 128)        0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 7296)              0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 128)               934016    \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 128)               0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 5)                 645       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1156741 (4.41 MB)\n",
      "Trainable params: 1156741 (4.41 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Compile the model\n",
    "model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',  # Use 'categorical_crossentropy' if one-hot encoding\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Print the model summary\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    preprocessed_data, preprocessed_labels, test_size=0.1, random_state=42\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3721"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_train)*len(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "49"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_test)*len(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4624"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(preprocessed_labels)*len(preprocessed_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Early stopping to avoid overfitting of model\n",
    "early_stop=EarlyStopping(monitor='val_accuracy',mode='max', verbose=1, patience=15, restore_best_weights=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "WARNING:tensorflow:From c:\\Users\\chada\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\keras\\src\\utils\\tf_utils.py:492: The name tf.ragged.RaggedTensorValue is deprecated. Please use tf.compat.v1.ragged.RaggedTensorValue instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\chada\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\keras\\src\\engine\\base_layer_utils.py:384: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n",
      "2/2 [==============================] - 3s 677ms/step - loss: 1.6159 - accuracy: 0.1967 - val_loss: 1.6255 - val_accuracy: 0.1429\n",
      "Epoch 2/30\n",
      "2/2 [==============================] - 0s 208ms/step - loss: 1.5591 - accuracy: 0.2623 - val_loss: 1.6531 - val_accuracy: 0.1429\n",
      "Epoch 3/30\n",
      "2/2 [==============================] - 0s 237ms/step - loss: 1.5546 - accuracy: 0.3443 - val_loss: 1.6201 - val_accuracy: 0.2857\n",
      "Epoch 4/30\n",
      "2/2 [==============================] - 0s 221ms/step - loss: 1.4892 - accuracy: 0.3770 - val_loss: 1.6813 - val_accuracy: 0.1429\n",
      "Epoch 5/30\n",
      "2/2 [==============================] - 0s 228ms/step - loss: 1.4460 - accuracy: 0.4098 - val_loss: 1.7427 - val_accuracy: 0.1429\n",
      "Epoch 6/30\n",
      "2/2 [==============================] - 0s 217ms/step - loss: 1.4020 - accuracy: 0.3770 - val_loss: 1.7353 - val_accuracy: 0.0000e+00\n",
      "Epoch 7/30\n",
      "2/2 [==============================] - 0s 185ms/step - loss: 1.3412 - accuracy: 0.5246 - val_loss: 1.8458 - val_accuracy: 0.2857\n",
      "Epoch 8/30\n",
      "2/2 [==============================] - 0s 239ms/step - loss: 1.2861 - accuracy: 0.4426 - val_loss: 2.0157 - val_accuracy: 0.2857\n",
      "Epoch 9/30\n",
      "2/2 [==============================] - 0s 206ms/step - loss: 1.2846 - accuracy: 0.4918 - val_loss: 1.8807 - val_accuracy: 0.1429\n",
      "Epoch 10/30\n",
      "2/2 [==============================] - 0s 211ms/step - loss: 1.1523 - accuracy: 0.5574 - val_loss: 2.2172 - val_accuracy: 0.4286\n",
      "Epoch 11/30\n",
      "2/2 [==============================] - 0s 239ms/step - loss: 1.2387 - accuracy: 0.4918 - val_loss: 1.8827 - val_accuracy: 0.1429\n",
      "Epoch 12/30\n",
      "2/2 [==============================] - 0s 248ms/step - loss: 1.0601 - accuracy: 0.6230 - val_loss: 1.7920 - val_accuracy: 0.0000e+00\n",
      "Epoch 13/30\n",
      "2/2 [==============================] - 0s 221ms/step - loss: 1.0580 - accuracy: 0.6557 - val_loss: 1.9805 - val_accuracy: 0.1429\n",
      "Epoch 14/30\n",
      "2/2 [==============================] - 0s 207ms/step - loss: 1.0278 - accuracy: 0.5902 - val_loss: 2.1413 - val_accuracy: 0.2857\n",
      "Epoch 15/30\n",
      "2/2 [==============================] - 0s 228ms/step - loss: 0.8706 - accuracy: 0.7213 - val_loss: 1.9168 - val_accuracy: 0.1429\n",
      "Epoch 16/30\n",
      "2/2 [==============================] - 0s 209ms/step - loss: 0.8231 - accuracy: 0.7213 - val_loss: 2.1044 - val_accuracy: 0.1429\n",
      "Epoch 17/30\n",
      "2/2 [==============================] - 0s 216ms/step - loss: 0.8701 - accuracy: 0.7213 - val_loss: 2.3796 - val_accuracy: 0.2857\n",
      "Epoch 18/30\n",
      "2/2 [==============================] - 0s 190ms/step - loss: 0.8439 - accuracy: 0.6230 - val_loss: 2.0781 - val_accuracy: 0.0000e+00\n",
      "Epoch 19/30\n",
      "2/2 [==============================] - 0s 193ms/step - loss: 0.7191 - accuracy: 0.7541 - val_loss: 2.1352 - val_accuracy: 0.0000e+00\n",
      "Epoch 20/30\n",
      "2/2 [==============================] - 0s 198ms/step - loss: 0.7015 - accuracy: 0.7869 - val_loss: 2.4467 - val_accuracy: 0.2857\n",
      "Epoch 21/30\n",
      "2/2 [==============================] - 0s 197ms/step - loss: 0.6184 - accuracy: 0.7705 - val_loss: 2.4186 - val_accuracy: 0.0000e+00\n",
      "Epoch 22/30\n",
      "2/2 [==============================] - 0s 191ms/step - loss: 0.4820 - accuracy: 0.8689 - val_loss: 2.3828 - val_accuracy: 0.0000e+00\n",
      "Epoch 23/30\n",
      "2/2 [==============================] - 0s 189ms/step - loss: 0.4906 - accuracy: 0.8033 - val_loss: 2.6433 - val_accuracy: 0.0000e+00\n",
      "Epoch 24/30\n",
      "2/2 [==============================] - 0s 186ms/step - loss: 0.3879 - accuracy: 0.8852 - val_loss: 2.9928 - val_accuracy: 0.0000e+00\n",
      "Epoch 25/30\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.4215 - accuracy: 0.8852Restoring model weights from the end of the best epoch: 10.\n",
      "2/2 [==============================] - 0s 202ms/step - loss: 0.4215 - accuracy: 0.8852 - val_loss: 3.0360 - val_accuracy: 0.0000e+00\n",
      "Epoch 25: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x1ab1b4c98b0>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit the model\n",
    "model.fit(X_train, y_train, epochs=30, batch_size=36,callbacks=[early_stop], validation_data=(X_test,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 0s 51ms/step - loss: 1.1267 - accuracy: 0.6230\n",
      "Training loss: 1.1267\n",
      "Training accuracy: 0.6230\n"
     ]
    }
   ],
   "source": [
    "loss, accuracy = model.evaluate(X_train, y_train)\n",
    "print(f\"Training loss: {loss:.4f}\")\n",
    "print(f\"Training accuracy: {accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 51ms/step - loss: 2.2172 - accuracy: 0.4286\n",
      "Test loss: 2.2172\n",
      "Test accuracy: 0.4286\n"
     ]
    }
   ],
   "source": [
    "loss, accuracy = model.evaluate(X_test, y_test)\n",
    "print(f\"Test loss: {loss:.4f}\")\n",
    "print(f\"Test accuracy: {accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "Test_preprocess_data = []\n",
    "Test_preprocess_label = []\n",
    "\n",
    "def Test_preprocess_audio(audio_file, label):\n",
    "    # Load audio\n",
    "    audio, sr = librosa.load(audio_file, sr=None)\n",
    "\n",
    "    # Perform feature extraction (e.g., MFCCs)\n",
    "    mfccs = librosa.feature.mfcc(y=audio, sr=sr, n_mfcc=num_mfcc_coefficients)\n",
    "\n",
    "    # Normalize the MFCCs (optional but recommended)\n",
    "    mfccs = (mfccs - np.mean(mfccs)) / np.std(mfccs)\n",
    "\n",
    "    # Reshape or pad the MFCCs to match the desired input shape\n",
    "    num_frames = mfccs.shape[1]\n",
    "    if num_frames < desired_shape[0]:\n",
    "        mfccs = np.pad(mfccs, ((0, 0), (0, desired_shape[0] - num_frames)), mode='constant')\n",
    "    elif num_frames > desired_shape[0]:\n",
    "        mfccs = mfccs[:, :desired_shape[0]]\n",
    "\n",
    "    # Append the preprocessed data and label\n",
    "    Test_preprocess_data.append(mfccs.T[:, :, np.newaxis])  # Transpose the data\n",
    "    Test_preprocess_label.append(label)\n",
    "\n",
    "    return np.array(Test_preprocess_data),np.array(Test_preprocess_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "##['belly_pain', 'burping', 'discomfort', 'hungry', 'tired']\n",
    "def Predict_Label(audio_file):\n",
    "    processed_data,processed_label = (Test_preprocess_audio(audio_file, label=None))\n",
    "    y_pred=model.predict(processed_data)\n",
    "    y_pred=np.argmax(y_pred,axis=1).any()\n",
    "    if y_pred == [0]:\n",
    "        print('belly_pain')\n",
    "    if y_pred == [1]:\n",
    "        print('burping')\n",
    "    if y_pred == [2]:\n",
    "        print('discomfort')\n",
    "    if y_pred == [3]:\n",
    "        print('hungry')\n",
    "    if y_pred == [4]:\n",
    "        print('tired')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 181ms/step\n",
      "burping\n"
     ]
    }
   ],
   "source": [
    "Predict_Label(\"donateacry/burping/F24DE44B-762C-4149-AC92-96A5E57ED118-1430816949-1.0-m-04-bu.wav\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\chada\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\keras\\src\\engine\\training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    }
   ],
   "source": [
    "model.save(\"Neonatal_cry_model.h5\", save_format=\"h5\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
